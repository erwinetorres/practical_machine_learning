---
title: "PML_Course Project"
author: "Erwin E. Torres"
date: "November 25, 2018"
output:
  html_document: default
  pdf_document: default
---
> ######### COURSE PROJECT ############
> 
> # Directory
> getwd()
[1] "C:/Users/Acer/Documents"
> setwd("C:/Users/Acer/Documents/DATA SCIENCE_DOST and COURSERA/Practical Machine Learning")
> 
> # Load necessary libraries
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(rattle)
Rattle: A free graphical interface for data science with R.
Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.
Type 'rattle()' to shake, rattle, and roll your data.
> library(rpart)
> library(RColorBrewer)
> library(randomForest)
randomForest 4.6-14
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:rattle’:

    importance

The following object is masked from ‘package:ggplot2’:

    margin

> library(gbm)
Loaded gbm 2.1.4
> 
> # Goal
> ## The goal of your project is to predict the manner in which they did the exercise. 
> ## This is the "classe" variable in the training set. You may use any of the other 
> ## variables to predict with. You should create a report describing how you built your model, 
> ## how you used cross validation, what you think the expected out of sample error is, 
> ## and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.
> 
> 
> # Load data from URL
> TrainingData <- read.csv("pml-training.csv")
> TestingData <- read.csv("pml-testing.csv")
> 
> 
> # Here we get the indexes of the columns having at least 90% of NA or blank values on the training dataset
> indColToRemove <- which(colSums(is.na(TrainingData) |TrainingData=="")>0.8*dim(TrainingData)[1]) 
> Clean_TrainingData <- TrainingData[,-indColToRemove]
> Clean_TrainingData <- Clean_TrainingData[,-c(1:7)] # Removing unecessary variables
> dim(Clean_TrainingData)
[1] 19622    53
> 
> # We do the same for the test set
> indColToRemove <- which(colSums(is.na(TestingData) |TestingData=="")>0.8*dim(TestingData)[1]) 
> Clean_TestingData <- TestingData[,-indColToRemove]
> Clean_TestingData <- Clean_TestingData[,-c(1:7)] # Removing unecessary variables
> dim(Clean_TestingData)
[1] 20 53
> 
> 
> # Creating a partition of the traning data set (TrainDataClean) 
> set.seed(12345)
> inTrain <- createDataPartition(Clean_TrainingData$classe, p=0.75, list=FALSE)
> Set_Training75 <- Clean_TrainingData[inTrain,] # Training data set, 75%
> Set_Testing25 <- Clean_TrainingData[-inTrain,] # Test data set, 25%
> 
> 
> # Train with classification trees
> Control_CT <- trainControl(method="cv", number=5)
> Model_CT <- train(classe~., data=Set_Training75, method="rpart", trControl=Control_CT)
> fancyRpartPlot(Model_CT$finalModel) #print(Model_CT)
> 
> # Predict using classification trees
> Pred_Training_CT <- predict(Model_CT,newdata=Set_Testing25)
> 
> # Display confusion matrix and model accuracy
> ConfMat_CT <- confusionMatrix(Set_Testing25$classe,Pred_Training_CT)
> ConfMat_CT$table
          Reference
Prediction   A   B   C   D   E
         A 870 159 273  88   5
         B 162 530 214  43   0
         C  29  36 674 116   0
         D  46 136 429 193   0
         E  16 221 224  51 389
> ConfMat_CT$overall[1]
 Accuracy 
0.5415987 
> 
> 
> # Train with random forests
> set.seed(12345)
> Model_RF <- randomForest(classe~., data=Set_Training75, importance=TRUE)
> print(Model_RF)

Call:
 randomForest(formula = classe ~ ., data = Set_Training75, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 0.41%
Confusion matrix:
     A    B    C    D    E  class.error
A 4184    1    0    0    0 0.0002389486
B    9 2836    3    0    0 0.0042134831
C    0   11 2553    3    0 0.0054538372
D    0    0   21 2388    3 0.0099502488
E    0    0    1    8 2697 0.0033259424
> 
> # Predict using random forests
> Pred_Training_RF <- predict(Model_RF,newdata=Set_Testing25)
> 
> # Display confusion matrix and model accuracy
> ConfMat_RF<- confusionMatrix(Set_Testing25$classe,Pred_Training_RF)
> ConfMat_RF$table
          Reference
Prediction    A    B    C    D    E
         A 1395    0    0    0    0
         B    5  939    5    0    0
         C    0    3  851    1    0
         D    0    0    7  797    0
         E    0    0    1    4  896
> ConfMat_RF$overall[1] # Shows the level of accuracy of the model
 Accuracy 
0.9946982 
> 
> 
> # Train with gradient boosting method
> set.seed(12345)
> Control_GBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
> Model_GBM  <- train(classe ~ ., data=Set_Training75, method = "gbm", trControl = Control_GBM, verbose = FALSE)
> print(Model_GBM)
> 
> # Predict using gradient boosting method
> Pred_Training_GBM <- predict(Model_GBM,newdata=Set_Testing25)
Error in predict(Model_GBM, newdata = Set_Testing25) : 
  object 'Model_GBM' not found
> 
> # Display confusion matrix and model accuracy
> ConfMat_GBM <- confusionMatrix(Set_Testing25$classe,Pred_Training_GBM)
Error in confusionMatrix(Set_Testing25$classe, Pred_Training_GBM) : 
  object 'Set_Testing25' not found
> ConfMat_GBM$table
Error: object 'ConfMat_GBM' not found
> ConfMat_GBM$overall[1]
Error: object 'ConfMat_GBM' not found
> 
> 
> # Conclusions
> Final_Pred_Test_RF<- predict(Model_RF,newdata=Clean_TestingData)
Error in predict(Model_RF, newdata = Clean_TestingData) : 
  object 'Model_RF' not found
 
> # Load data from URL
> TrainingData <- read.csv("pml-training.csv")
> TrainingData <- read.csv("pml-training.csv")
> TestingData <- read.csv("pml-testing.csv")
> TestingData <- read.csv("pml-testing.csv")
> getwd()

[1] "C:/Users/Acer/Documents/DATA SCIENCE_DOST and COURSERA/Practical Machine Learning"
> # Get the indexes of the columns having at least 80% of NA or blank values on the training dataset
> indColToRemove <- which(colSums(is.na(TrainingData) |TrainingData=="")>0.8*dim(TrainingData)[1])
> Clean_TrainingData <- TrainingData[,-indColToRemove]
> setwd("C:/Users/Acer/Documents/DATA SCIENCE_DOST and COURSERA/Practical Machine Learning")
> library(caret)
> library(rattle)
> library(rpart)
> library(RColorBrewer)
> library(randomForest)
> library(gbm)
> Clean_TrainingData <- Clean_TrainingData[,-c(1:7)] # Removing unecessary variables
> dim(Clean_TrainingData)
[1] 19622    53
> # We do the same for the test set
> indColToRemove <- which(colSums(is.na(TestingData) |TestingData=="")>0.8*dim(TestingData)[1])
> Clean_TestingData <- TestingData[,-indColToRemove]
> Clean_TestingData <- Clean_TestingData[,-c(1:7)] # Removing unecessary variables
> dim(Clean_TestingData)
[1] 20 53
> # Create a partition of the traning data set (TrainDataClean) 
> set.seed(12345)
> inTrain <- createDataPartition(Clean_TrainingData$classe, p=0.75, list=FALSE)
> Set_Training75 <- Clean_TrainingData[inTrain,] # Training data set, 75%
> Set_Testing25 <- Clean_TrainingData[-inTrain,] # Test data set, 25%
> Model_GBM  <- train(classe ~ ., data=Set_Training75, method = "gbm", trControl = Control_GBM, verbose = FALSE)
Error in train.default(x, y, weights = w, ...) : 
  object 'Control_GBM' not found
> # Train with gradient boosting method
> set.seed(12345)
> Control_GBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
> Model_GBM  <- train(classe ~ ., data=Set_Training75, method = "gbm", trControl = Control_GBM, verbose = FALSE)
> Model_GBM$finalModel
A gradient boosted model with multinomial loss function.
150 iterations were performed.
There were 52 predictors of which 41 had non-zero influence.
> print(Model_GBM)
Stochastic Gradient Boosting 

14718 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 1 times) 
Summary of sample sizes: 11774, 11774, 11774, 11775, 11775 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.7564204  0.6912841
  1                  100      0.8198803  0.7719731
  1                  150      0.8549392  0.8164228
  2                   50      0.8549389  0.8161407
  2                  100      0.9075278  0.8829416
  2                  150      0.9316484  0.9134928
  3                   50      0.8952979  0.8674514
  3                  100      0.9424511  0.9271878
  3                  150      0.9618833  0.9517747

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning parameter 'n.minobsinnode'
 was held constant at a value of 10
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage = 0.1
 and n.minobsinnode = 10.
> # Predict using gradient boosting method
> Pred_Training_GBM <- predict(Model_GBM,newdata=Set_Testing25)
> # Display confusion matrix and model accuracy
> ConfMat_GBM <- confusionMatrix(Set_Testing25$classe,Pred_Training_GBM)
> ConfMat_GBM$table
          Reference
Prediction    A    B    C    D    E
         A 1378   12    4    0    1
         B   37  880   32    0    0
         C    0   18  824   12    1
         D    1    1   27  770    5
         E    2    8   12   17  862
> ConfMat_GBM$overall[1]
 Accuracy 
0.9612561 
> # Conclusions
> Final_Pred_Test_RF<- predict(Model_RF,newdata=Clean_TestingData)
Error in predict(Model_RF, newdata = Clean_TestingData) : 
  object 'Model_RF' not found
> # Train with random forests
> set.seed(12345)
> getwd()
[1] "C:/Users/Acer/Documents/DATA SCIENCE_DOST and COURSERA/Practical Machine Learning"
> setwd("C:/Users/Acer/Documents/DATA SCIENCE_DOST and COURSERA/Practical Machine Learning")
> library(caret)
> library(rattle)
> library(rpart)
> library(RColorBrewer)
> library(randomForest)
> library(gbm)
> TrainingData <- read.csv("pml-training.csv")
> TestingData <- read.csv("pml-testing.csv")
> str(TrainingData)
'data.frame':	19622 obs. of  160 variables:
 $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...
 $ user_name               : Factor w/ 6 levels "adelmo","carlitos",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ raw_timestamp_part_1    : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...
 $ raw_timestamp_part_2    : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...
 $ cvtd_timestamp          : Factor w/ 20 levels "02/12/2011 13:32",..: 9 9 9 9 9 9 9 9 9 9 ...
 $ new_window              : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...
 $ num_window              : int  11 11 11 12 12 12 12 12 12 12 ...
 $ roll_belt               : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...
 $ pitch_belt              : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...
 $ yaw_belt                : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...
 $ total_accel_belt        : int  3 3 3 3 3 3 3 3 3 3 ...
 $ kurtosis_roll_belt      : Factor w/ 397 levels "","-0.016850",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ kurtosis_picth_belt     : Factor w/ 317 levels "","-0.021887",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ kurtosis_yaw_belt       : Factor w/ 2 levels "","#DIV/0!": 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_roll_belt      : Factor w/ 395 levels "","-0.003095",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_roll_belt.1    : Factor w/ 338 levels "","-0.005928",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_yaw_belt       : Factor w/ 2 levels "","#DIV/0!": 1 1 1 1 1 1 1 1 1 1 ...
 $ max_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ max_picth_belt          : int  NA NA NA NA NA NA NA NA NA NA ...
 $ max_yaw_belt            : Factor w/ 68 levels "","-0.1","-0.2",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ min_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ min_pitch_belt          : int  NA NA NA NA NA NA NA NA NA NA ...
 $ min_yaw_belt            : Factor w/ 68 levels "","-0.1","-0.2",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ amplitude_roll_belt     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ amplitude_pitch_belt    : int  NA NA NA NA NA NA NA NA NA NA ...
 $ amplitude_yaw_belt      : Factor w/ 4 levels "","#DIV/0!","0.00",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ var_total_accel_belt    : num  NA NA NA NA NA NA NA NA NA NA ...
 $ avg_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ stddev_roll_belt        : num  NA NA NA NA NA NA NA NA NA NA ...
 $ var_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ avg_pitch_belt          : num  NA NA NA NA NA NA NA NA NA NA ...
 $ stddev_pitch_belt       : num  NA NA NA NA NA NA NA NA NA NA ...
 $ var_pitch_belt          : num  NA NA NA NA NA NA NA NA NA NA ...
 $ avg_yaw_belt            : num  NA NA NA NA NA NA NA NA NA NA ...
 $ stddev_yaw_belt         : num  NA NA NA NA NA NA NA NA NA NA ...
 $ var_yaw_belt            : num  NA NA NA NA NA NA NA NA NA NA ...
 $ gyros_belt_x            : num  0 0.02 0 0.02 0.02 0.02 0.02 0.02 0.02 0.03 ...
 $ gyros_belt_y            : num  0 0 0 0 0.02 0 0 0 0 0 ...
 $ gyros_belt_z            : num  -0.02 -0.02 -0.02 -0.03 -0.02 -0.02 -0.02 -0.02 -0.02 0 ...
 $ accel_belt_x            : int  -21 -22 -20 -22 -21 -21 -22 -22 -20 -21 ...
 $ accel_belt_y            : int  4 4 5 3 2 4 3 4 2 4 ...
 $ accel_belt_z            : int  22 22 23 21 24 21 21 21 24 22 ...
 $ magnet_belt_x           : int  -3 -7 -2 -6 -6 0 -4 -2 1 -3 ...
 $ magnet_belt_y           : int  599 608 600 604 600 603 599 603 602 609 ...
 $ magnet_belt_z           : int  -313 -311 -305 -310 -302 -312 -311 -313 -312 -308 ...
 $ roll_arm                : num  -128 -128 -128 -128 -128 -128 -128 -128 -128 -128 ...
 $ pitch_arm               : num  22.5 22.5 22.5 22.1 22.1 22 21.9 21.8 21.7 21.6 ...
 $ yaw_arm                 : num  -161 -161 -161 -161 -161 -161 -161 -161 -161 -161 ...
 $ total_accel_arm         : int  34 34 34 34 34 34 34 34 34 34 ...
 $ var_accel_arm           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ avg_roll_arm            : num  NA NA NA NA NA NA NA NA NA NA ...
 $ stddev_roll_arm         : num  NA NA NA NA NA NA NA NA NA NA ...
 $ var_roll_arm            : num  NA NA NA NA NA NA NA NA NA NA ...
 $ avg_pitch_arm           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ stddev_pitch_arm        : num  NA NA NA NA NA NA NA NA NA NA ...
 $ var_pitch_arm           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ avg_yaw_arm             : num  NA NA NA NA NA NA NA NA NA NA ...
 $ stddev_yaw_arm          : num  NA NA NA NA NA NA NA NA NA NA ...
 $ var_yaw_arm             : num  NA NA NA NA NA NA NA NA NA NA ...
 $ gyros_arm_x             : num  0 0.02 0.02 0.02 0 0.02 0 0.02 0.02 0.02 ...
 $ gyros_arm_y             : num  0 -0.02 -0.02 -0.03 -0.03 -0.03 -0.03 -0.02 -0.03 -0.03 ...
 $ gyros_arm_z             : num  -0.02 -0.02 -0.02 0.02 0 0 0 0 -0.02 -0.02 ...
 $ accel_arm_x             : int  -288 -290 -289 -289 -289 -289 -289 -289 -288 -288 ...
 $ accel_arm_y             : int  109 110 110 111 111 111 111 111 109 110 ...
 $ accel_arm_z             : int  -123 -125 -126 -123 -123 -122 -125 -124 -122 -124 ...
 $ magnet_arm_x            : int  -368 -369 -368 -372 -374 -369 -373 -372 -369 -376 ...
 $ magnet_arm_y            : int  337 337 344 344 337 342 336 338 341 334 ...
 $ magnet_arm_z            : int  516 513 513 512 506 513 509 510 518 516 ...
 $ kurtosis_roll_arm       : Factor w/ 330 levels "","-0.02438",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ kurtosis_picth_arm      : Factor w/ 328 levels "","-0.00484",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ kurtosis_yaw_arm        : Factor w/ 395 levels "","-0.01548",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_roll_arm       : Factor w/ 331 levels "","-0.00051",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_pitch_arm      : Factor w/ 328 levels "","-0.00184",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_yaw_arm        : Factor w/ 395 levels "","-0.00311",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ max_roll_arm            : num  NA NA NA NA NA NA NA NA NA NA ...
 $ max_picth_arm           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ max_yaw_arm             : int  NA NA NA NA NA NA NA NA NA NA ...
 $ min_roll_arm            : num  NA NA NA NA NA NA NA NA NA NA ...
 $ min_pitch_arm           : num  NA NA NA NA NA NA NA NA NA NA ...
 $ min_yaw_arm             : int  NA NA NA NA NA NA NA NA NA NA ...
 $ amplitude_roll_arm      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ amplitude_pitch_arm     : num  NA NA NA NA NA NA NA NA NA NA ...
 $ amplitude_yaw_arm       : int  NA NA NA NA NA NA NA NA NA NA ...
 $ roll_dumbbell           : num  13.1 13.1 12.9 13.4 13.4 ...
 $ pitch_dumbbell          : num  -70.5 -70.6 -70.3 -70.4 -70.4 ...
 $ yaw_dumbbell            : num  -84.9 -84.7 -85.1 -84.9 -84.9 ...
 $ kurtosis_roll_dumbbell  : Factor w/ 398 levels "","-0.0035","-0.0073",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ kurtosis_picth_dumbbell : Factor w/ 401 levels "","-0.0163","-0.0233",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ kurtosis_yaw_dumbbell   : Factor w/ 2 levels "","#DIV/0!": 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_roll_dumbbell  : Factor w/ 401 levels "","-0.0082","-0.0096",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_pitch_dumbbell : Factor w/ 402 levels "","-0.0053","-0.0084",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ skewness_yaw_dumbbell   : Factor w/ 2 levels "","#DIV/0!": 1 1 1 1 1 1 1 1 1 1 ...
 $ max_roll_dumbbell       : num  NA NA NA NA NA NA NA NA NA NA ...
 $ max_picth_dumbbell      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ max_yaw_dumbbell        : Factor w/ 73 levels "","-0.1","-0.2",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ min_roll_dumbbell       : num  NA NA NA NA NA NA NA NA NA NA ...
 $ min_pitch_dumbbell      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ min_yaw_dumbbell        : Factor w/ 73 levels "","-0.1","-0.2",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ amplitude_roll_dumbbell : num  NA NA NA NA NA NA NA NA NA NA ...
  [list output truncated]
> str(TestingData)
'data.frame':	20 obs. of  160 variables:
 $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...
 $ user_name               : Factor w/ 6 levels "adelmo","carlitos",..: 6 5 5 1 4 5 5 5 2 3 ...
 $ raw_timestamp_part_1    : int  1323095002 1322673067 1322673075 1322832789 1322489635 1322673149 1322673128 1322673076 1323084240 1322837822 ...
 $ raw_timestamp_part_2    : int  868349 778725 342967 560311 814776 510661 766645 54671 916313 384285 ...
 $ cvtd_timestamp          : Factor w/ 11 levels "02/12/2011 13:33",..: 5 10 10 1 6 11 11 10 3 2 ...
 $ new_window              : Factor w/ 1 level "no": 1 1 1 1 1 1 1 1 1 1 ...
 $ num_window              : int  74 431 439 194 235 504 485 440 323 664 ...
 $ roll_belt               : num  123 1.02 0.87 125 1.35 -5.92 1.2 0.43 0.93 114 ...
 $ pitch_belt              : num  27 4.87 1.82 -41.6 3.33 1.59 4.44 4.15 6.72 22.4 ...
 $ yaw_belt                : num  -4.75 -88.9 -88.5 162 -88.6 -87.7 -87.3 -88.5 -93.7 -13.1 ...
 $ total_accel_belt        : int  20 4 5 17 3 4 4 4 4 18 ...
 $ kurtosis_roll_belt      : logi  NA NA NA NA NA NA ...
 $ kurtosis_picth_belt     : logi  NA NA NA NA NA NA ...
 $ kurtosis_yaw_belt       : logi  NA NA NA NA NA NA ...
 $ skewness_roll_belt      : logi  NA NA NA NA NA NA ...
 $ skewness_roll_belt.1    : logi  NA NA NA NA NA NA ...
 $ skewness_yaw_belt       : logi  NA NA NA NA NA NA ...
 $ max_roll_belt           : logi  NA NA NA NA NA NA ...
 $ max_picth_belt          : logi  NA NA NA NA NA NA ...
 $ max_yaw_belt            : logi  NA NA NA NA NA NA ...
 $ min_roll_belt           : logi  NA NA NA NA NA NA ...
 $ min_pitch_belt          : logi  NA NA NA NA NA NA ...
 $ min_yaw_belt            : logi  NA NA NA NA NA NA ...
 $ amplitude_roll_belt     : logi  NA NA NA NA NA NA ...
 $ amplitude_pitch_belt    : logi  NA NA NA NA NA NA ...
 $ amplitude_yaw_belt      : logi  NA NA NA NA NA NA ...
 $ var_total_accel_belt    : logi  NA NA NA NA NA NA ...
 $ avg_roll_belt           : logi  NA NA NA NA NA NA ...
 $ stddev_roll_belt        : logi  NA NA NA NA NA NA ...
 $ var_roll_belt           : logi  NA NA NA NA NA NA ...
 $ avg_pitch_belt          : logi  NA NA NA NA NA NA ...
 $ stddev_pitch_belt       : logi  NA NA NA NA NA NA ...
 $ var_pitch_belt          : logi  NA NA NA NA NA NA ...
 $ avg_yaw_belt            : logi  NA NA NA NA NA NA ...
 $ stddev_yaw_belt         : logi  NA NA NA NA NA NA ...
 $ var_yaw_belt            : logi  NA NA NA NA NA NA ...
 $ gyros_belt_x            : num  -0.5 -0.06 0.05 0.11 0.03 0.1 -0.06 -0.18 0.1 0.14 ...
 $ gyros_belt_y            : num  -0.02 -0.02 0.02 0.11 0.02 0.05 0 -0.02 0 0.11 ...
 $ gyros_belt_z            : num  -0.46 -0.07 0.03 -0.16 0 -0.13 0 -0.03 -0.02 -0.16 ...
 $ accel_belt_x            : int  -38 -13 1 46 -8 -11 -14 -10 -15 -25 ...
 $ accel_belt_y            : int  69 11 -1 45 4 -16 2 -2 1 63 ...
 $ accel_belt_z            : int  -179 39 49 -156 27 38 35 42 32 -158 ...
 $ magnet_belt_x           : int  -13 43 29 169 33 31 50 39 -6 10 ...
 $ magnet_belt_y           : int  581 636 631 608 566 638 622 635 600 601 ...
 $ magnet_belt_z           : int  -382 -309 -312 -304 -418 -291 -315 -305 -302 -330 ...
 $ roll_arm                : num  40.7 0 0 -109 76.1 0 0 0 -137 -82.4 ...
 $ pitch_arm               : num  -27.8 0 0 55 2.76 0 0 0 11.2 -63.8 ...
 $ yaw_arm                 : num  178 0 0 -142 102 0 0 0 -167 -75.3 ...
 $ total_accel_arm         : int  10 38 44 25 29 14 15 22 34 32 ...
 $ var_accel_arm           : logi  NA NA NA NA NA NA ...
 $ avg_roll_arm            : logi  NA NA NA NA NA NA ...
 $ stddev_roll_arm         : logi  NA NA NA NA NA NA ...
 $ var_roll_arm            : logi  NA NA NA NA NA NA ...
 $ avg_pitch_arm           : logi  NA NA NA NA NA NA ...
 $ stddev_pitch_arm        : logi  NA NA NA NA NA NA ...
 $ var_pitch_arm           : logi  NA NA NA NA NA NA ...
 $ avg_yaw_arm             : logi  NA NA NA NA NA NA ...
 $ stddev_yaw_arm          : logi  NA NA NA NA NA NA ...
 $ var_yaw_arm             : logi  NA NA NA NA NA NA ...
 $ gyros_arm_x             : num  -1.65 -1.17 2.1 0.22 -1.96 0.02 2.36 -3.71 0.03 0.26 ...
 $ gyros_arm_y             : num  0.48 0.85 -1.36 -0.51 0.79 0.05 -1.01 1.85 -0.02 -0.5 ...
 $ gyros_arm_z             : num  -0.18 -0.43 1.13 0.92 -0.54 -0.07 0.89 -0.69 -0.02 0.79 ...
 $ accel_arm_x             : int  16 -290 -341 -238 -197 -26 99 -98 -287 -301 ...
 $ accel_arm_y             : int  38 215 245 -57 200 130 79 175 111 -42 ...
 $ accel_arm_z             : int  93 -90 -87 6 -30 -19 -67 -78 -122 -80 ...
 $ magnet_arm_x            : int  -326 -325 -264 -173 -170 396 702 535 -367 -420 ...
 $ magnet_arm_y            : int  385 447 474 257 275 176 15 215 335 294 ...
 $ magnet_arm_z            : int  481 434 413 633 617 516 217 385 520 493 ...
 $ kurtosis_roll_arm       : logi  NA NA NA NA NA NA ...
 $ kurtosis_picth_arm      : logi  NA NA NA NA NA NA ...
 $ kurtosis_yaw_arm        : logi  NA NA NA NA NA NA ...
 $ skewness_roll_arm       : logi  NA NA NA NA NA NA ...
 $ skewness_pitch_arm      : logi  NA NA NA NA NA NA ...
 $ skewness_yaw_arm        : logi  NA NA NA NA NA NA ...
 $ max_roll_arm            : logi  NA NA NA NA NA NA ...
 $ max_picth_arm           : logi  NA NA NA NA NA NA ...
 $ max_yaw_arm             : logi  NA NA NA NA NA NA ...
 $ min_roll_arm            : logi  NA NA NA NA NA NA ...
 $ min_pitch_arm           : logi  NA NA NA NA NA NA ...
 $ min_yaw_arm             : logi  NA NA NA NA NA NA ...
 $ amplitude_roll_arm      : logi  NA NA NA NA NA NA ...
 $ amplitude_pitch_arm     : logi  NA NA NA NA NA NA ...
 $ amplitude_yaw_arm       : logi  NA NA NA NA NA NA ...
 $ roll_dumbbell           : num  -17.7 54.5 57.1 43.1 -101.4 ...
 $ pitch_dumbbell          : num  25 -53.7 -51.4 -30 -53.4 ...
 $ yaw_dumbbell            : num  126.2 -75.5 -75.2 -103.3 -14.2 ...
 $ kurtosis_roll_dumbbell  : logi  NA NA NA NA NA NA ...
 $ kurtosis_picth_dumbbell : logi  NA NA NA NA NA NA ...
 $ kurtosis_yaw_dumbbell   : logi  NA NA NA NA NA NA ...
 $ skewness_roll_dumbbell  : logi  NA NA NA NA NA NA ...
 $ skewness_pitch_dumbbell : logi  NA NA NA NA NA NA ...
 $ skewness_yaw_dumbbell   : logi  NA NA NA NA NA NA ...
 $ max_roll_dumbbell       : logi  NA NA NA NA NA NA ...
 $ max_picth_dumbbell      : logi  NA NA NA NA NA NA ...
 $ max_yaw_dumbbell        : logi  NA NA NA NA NA NA ...
 $ min_roll_dumbbell       : logi  NA NA NA NA NA NA ...
 $ min_pitch_dumbbell      : logi  NA NA NA NA NA NA ...
 $ min_yaw_dumbbell        : logi  NA NA NA NA NA NA ...
 $ amplitude_roll_dumbbell : logi  NA NA NA NA NA NA ...
  [list output truncated]
> indColToRemove <- which(colSums(is.na(TrainingData) |TrainingData=="")>0.8*dim(TrainingData)[1]) 
> Clean_TrainingData <- TrainingData[,-indColToRemove]
> Clean_TrainingData <- Clean_TrainingData[,-c(1:7)] # Removing unecessary variables
> dim(Clean_TrainingData)
[1] 19622    53
> 
> indColToRemove <- which(colSums(is.na(TestingData) |TestingData=="")>0.8*dim(TestingData)[1]) 
> Clean_TestingData <- TestingData[,-indColToRemove]
> Clean_TestingData <- Clean_TestingData[,-c(1:7)] # Removing unecessary variables
> dim(Clean_TestingData)
[1] 20 53
> 
> set.seed(12345)
> inTrain <- createDataPartition(Clean_TrainingData$classe, p=0.75, list=FALSE)
> Set_Training75 <- Clean_TrainingData[inTrain,] # Training data set, 75%
> Set_Testing25 <- Clean_TrainingData[-inTrain,] # Test data set, 25%
> Control_CT <- trainControl(method="cv", number=5)
> Model_CT <- train(classe~., data=Set_Training75, method="rpart", trControl=Control_CT)
> fancyRpartPlot(Model_CT$finalModel) #print(Model_CT)
> Pred_Training_CT <- predict(Model_CT,newdata=Set_Testing25)
> ConfMat_CT <- confusionMatrix(Set_Testing25$classe,Pred_Training_CT)
> ConfMat_CT$table
          Reference
Prediction   A   B   C   D   E
         A 870 159 273  88   5
         B 162 530 214  43   0
         C  29  36 674 116   0
         D  46 136 429 193   0
         E  16 221 224  51 389
> ConfMat_CT$overall[1]
 Accuracy 
0.5415987 
> set.seed(12345)
> Model_RF <- randomForest(classe~., data=Set_Training75, importance=TRUE)
> print(Model_RF)

Call:
 randomForest(formula = classe ~ ., data = Set_Training75, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 0.41%
Confusion matrix:
     A    B    C    D    E  class.error
A 4184    1    0    0    0 0.0002389486
B    9 2836    3    0    0 0.0042134831
C    0   11 2553    3    0 0.0054538372
D    0    0   21 2388    3 0.0099502488
E    0    0    1    8 2697 0.0033259424
> Pred_Training_RF <- predict(Model_RF,newdata=Set_Testing25)
> ConfMat_RF<- confusionMatrix(Set_Testing25$classe,Pred_Training_RF)
> ConfMat_RF$table
          Reference
Prediction    A    B    C    D    E
         A 1395    0    0    0    0
         B    5  939    5    0    0
         C    0    3  851    1    0
         D    0    0    7  797    0
         E    0    0    1    4  896
> ConfMat_RF$overall[1] # Shows the level of accuracy of the model
 Accuracy 
0.9946982 
> set.seed(12345)
> Control_GBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
> Model_GBM  <- train(classe ~ ., data=Set_Training75, method = "gbm", trControl = Control_GBM, verbose = FALSE)
> Model_GBM$finalModel
A gradient boosted model with multinomial loss function.
150 iterations were performed.
There were 52 predictors of which 41 had non-zero influence.
> print(Model_GBM)
Stochastic Gradient Boosting 

14718 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 1 times) 
Summary of sample sizes: 11774, 11774, 11774, 11775, 11775 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.7564204  0.6912841
  1                  100      0.8198803  0.7719731
  1                  150      0.8549392  0.8164228
  2                   50      0.8549389  0.8161407
  2                  100      0.9075278  0.8829416
  2                  150      0.9316484  0.9134928
  3                   50      0.8952979  0.8674514
  3                  100      0.9424511  0.9271878
  3                  150      0.9618833  0.9517747

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning
 parameter 'n.minobsinnode' was held constant at a value of 10
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage =
 0.1 and n.minobsinnode = 10.
> Pred_Training_GBM <- predict(Model_GBM,newdata=Set_Testing25)
> ConfMat_GBM <- confusionMatrix(Set_Testing25$classe,Pred_Training_GBM)
> ConfMat_GBM$table
          Reference
Prediction    A    B    C    D    E
         A 1378   12    4    0    1
         B   37  880   32    0    0
         C    0   18  824   12    1
         D    1    1   27  770    5
         E    2    8   12   17  862
> ConfMat_GBM$overall[1]
 Accuracy 
0.9612561 
> Final_Pred_Test_RF<- predict(Model_RF,newdata=Clean_TestingData)
> Final_Pred_Test_RF
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
 B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
Levels: A B C D E
